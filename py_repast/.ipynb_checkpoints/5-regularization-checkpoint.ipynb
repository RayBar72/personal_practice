{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47ac6f7",
   "metadata": {},
   "source": [
    "# 0. L2 Regularization Cost\n",
    "Write a function `def l2_reg_cost(cost, lambtha, weights, L, m):` that calculates the cost of a neural network with L2 regularization:\n",
    "\n",
    "- `cost` is the cost of the network without L2 regularization\n",
    "- `lambtha` is the regularization parameter\n",
    "- `weights` is a dictionary of the weights and biases (`numpy.ndarrays`) of the neural network\n",
    "- `L` is the number of layers in the neural network\n",
    "- `m` is the number of data points used\n",
    "- Returns: the cost of the network accounting for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009cc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def l2_reg_cost(cost, lambtha, weights, L, m):\n",
    "    W = [weights['W' + str(i + 1)] for i in range(L)]\n",
    "    W = [np.linalg.norm(w) ** 2 for w in W]\n",
    "    c = cost + lambtha * sum(W) / (2 * m)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868fb2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41842822]\n",
      "[12.11229237]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "\n",
    "    cost = np.abs(np.random.randn(1))\n",
    "\n",
    "    print(cost)\n",
    "    cost = l2_reg_cost(cost, 0.1, weights, 3, 1000)\n",
    "    print(cost)\n",
    "#     W = [weights['W' + str(i + 1)] for i in range(3)]\n",
    "#     print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d34c4",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent with L2 Regularization\n",
    "Write a function `def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):` that updates the weights and biases of a neural network using gradient descent with L2 regularization:\n",
    "\n",
    "* `Y` is a one-hot `numpy.ndarray` of shape (`classes, m`) that contains the correct labels for the data\n",
    "    * `classes` is the number of classes\n",
    "    * `m` is the number of data points\n",
    "* `weights` is a dictionary of the weights and biases of the neural network\n",
    "* `cache` is a dictionary of the outputs of each layer of the neural network\n",
    "* `alpha` is the learning rate\n",
    "* `lambtha` is the L2 regularization parameter\n",
    "* `L` is the number of layers of the network\n",
    "* The neural network uses `tanh` activations on each layer except the last, which uses a `softmax` activation\n",
    "* The weights and biases of the network should be updated in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107e9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that updates the weights and biases of a neural\n",
    "network using gradient descent with L2 regularization\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):\n",
    "    '''\n",
    "    Function that updates the weights and biases of a neural\n",
    "    network using gradient descent with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : TYPE numpy.ndarray\n",
    "        DESCRIPTION. Y is a one-hot numpy.ndarray of shape (classes, m)\n",
    "        that contains the correct labels for the data\n",
    "    weights : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the weights and biases of the\n",
    "        neural network\n",
    "    cache : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the outputs of each layer of\n",
    "        the neural network\n",
    "    alpha : TYPE float\n",
    "        DESCRIPTION. Learning rate\n",
    "    lambtha : TYPE float\n",
    "        DESCRIPTION. L2 regularization parameter\n",
    "    L : TYPE int\n",
    "        DESCRIPTION. layers of the network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    for i in reversed(range(L)):\n",
    "        w = 'W' + str(i + 1)\n",
    "        b = 'b' + str(i + 1)\n",
    "        a = 'A' + str(i + 1)\n",
    "        a_0 = 'A' + str(i)\n",
    "        A = cache[a]\n",
    "        A_dw = cache[a_0]\n",
    "        if i == L - 1:\n",
    "            dz = A - Y\n",
    "            W = weights[w]\n",
    "        else:\n",
    "            da = 1 - (A * A)\n",
    "            dz = np.matmul(W.T, dz)\n",
    "            dz = dz * da\n",
    "            W = weights[w]\n",
    "        dw = np.matmul(A_dw, dz.T) / m\n",
    "        db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "        weights[w] = weights[w] - alpha * (dw.T + (lambtha / m * weights[w]))\n",
    "        weights[b] = weights[b] - alpha * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4709a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719\n",
      "  -1.34149673]\n",
      " [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133\n",
      "   0.07912172]\n",
      " [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445\n",
      "  -1.07836109]\n",
      " ...\n",
      " [-0.60467085  0.54751161 -1.23317415 ...  0.82895532  1.44161136\n",
      "   0.18972404]\n",
      " [-0.41044606  0.85719512  0.71789835 ... -0.73954771  0.5074628\n",
      "   1.23022874]\n",
      " [ 0.43129249  0.60767018 -0.07749988 ... -0.26611561  2.52287972\n",
      "   0.73131543]]\n",
      "[[ 1.76405199  0.40015713  0.97873779 ...  0.52130364  0.61192707\n",
      "  -1.34149646]\n",
      " [ 0.47689827  0.14844955  0.52904513 ...  0.09600419 -0.04511329\n",
      "   0.07912171]\n",
      " [ 0.85053051 -0.83912402 -1.01177388 ... -0.07223874  0.31112438\n",
      "  -1.07836088]\n",
      " ...\n",
      " [-0.60467073  0.5475115  -1.2331739  ...  0.82895516  1.44161107\n",
      "   0.189724  ]\n",
      " [-0.41044598  0.85719495  0.71789821 ... -0.73954756  0.5074627\n",
      "   1.2302285 ]\n",
      " [ 0.4312924   0.60767006 -0.07749987 ... -0.26611556  2.52287922\n",
      "   0.73131529]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "# l2_reg_gradient_descent = __import__('1-l2_reg_gradient_descent').l2_reg_gradient_descent\n",
    "\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "    cache = {}\n",
    "    cache['A0'] = X_train\n",
    "    cache['A1'] = np.tanh(np.matmul(weights['W1'], cache['A0']) + weights['b1'])\n",
    "    cache['A2'] = np.tanh(np.matmul(weights['W2'], cache['A1']) + weights['b2'])\n",
    "    Z3 = np.matmul(weights['W3'], cache['A2']) + weights['b3']\n",
    "    cache['A3'] = np.exp(Z3) / np.sum(np.exp(Z3), axis=0)\n",
    "    print(weights['W1'])\n",
    "    l2_reg_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.1, 3)\n",
    "    print(weights['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa182c9",
   "metadata": {},
   "source": [
    "# 2. L2 Regularization Cost\n",
    "Write the function `def l2_reg_cost(cost):` that calculates the cost of a neural network with L2 regularization:\n",
    "\n",
    "* `cost` is a tensor containing the cost of the network without L2 regularization\n",
    "* Returns: a tensor containing the cost of the network accounting for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd3cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Moudulus that calculates the cost of a NN with l2 regularization\n",
    "'''\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "def l2_reg_cost(cost):\n",
    "    '''\n",
    "    Function that calculates the cost of a neural network\n",
    "    with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cost : TYPE tensor\n",
    "        DESCRIPTION. Tensor containing the cost\n",
    "        of the network without L2 regularization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tensor containing the cost of the network accounting for\n",
    "    L2 regularization.\n",
    "\n",
    "    '''\n",
    "    cost = cost + tf.losses.get_regularization_loss()\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1746e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Moudulus that calculates the cost of a NN with l2 regularization\n",
    "'''\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "def l2_reg_cost(cost):\n",
    "    '''\n",
    "    Function that calculates the cost of a neural network\n",
    "    with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cost : TYPE tensor\n",
    "        DESCRIPTION. Tensor containing the cost\n",
    "        of the network without L2 regularization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tensor containing the cost of the network accounting for\n",
    "    L2 regularization.\n",
    "\n",
    "    '''\n",
    "    return cost + tf.losses.get_regularization_losses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49dfc7",
   "metadata": {},
   "source": [
    "## Ojo\n",
    "* En los recursos leidos se tomaba get_regularization_loss. Esto genera un vector 1x1\n",
    "* En el video de resoluciÃ³n se utiliza get_regularization_loss**es** esto arroja un vector de 1x3\n",
    "* Si se corre varias veces el programa, la memoria va aumentando el tamaÃ±o del vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10c3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.430164 49.647583  6.044121]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "# l2_reg_cost = __import__('2-l2_reg_cost').l2_reg_cost\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    oh = np.zeros((classes, m))\n",
    "    oh[Y, np.arange(m)] = 1\n",
    "    return oh\n",
    "\n",
    "np.random.seed(4)\n",
    "m = np.random.randint(1000, 2000)\n",
    "c = 10\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "\n",
    "X = lib['X_train'][:m].reshape((m, -1))\n",
    "Y = one_hot(lib['Y_train'][:m], c).T\n",
    "\n",
    "n0 = X.shape[1]\n",
    "n1, n2 = np.random.randint(10, 1000, 2)\n",
    "\n",
    "lam = 0.09\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, n0))\n",
    "y = tf.placeholder(tf.float32, (None, c))\n",
    "\n",
    "a1 = tf.layers.Dense(n1, activation=tf.nn.tanh, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode=(\"fan_avg\")), kernel_regularizer=tf.keras.regularizers.L2(lam))(x)\n",
    "a2 = tf.layers.Dense(n2, activation=tf.nn.sigmoid, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode=(\"fan_avg\")), kernel_regularizer=tf.keras.regularizers.L2(lam))(a1)\n",
    "y_pred = tf.layers.Dense(c, activation=None, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode=(\"fan_avg\")), kernel_regularizer=tf.keras.regularizers.L2(lam))(a2)\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "\n",
    "l2_cost = l2_reg_cost(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(l2_cost, feed_dict={x: X, y: Y}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060e3e3",
   "metadata": {},
   "source": [
    "# 3. Create a Layer with L2 Regularization\n",
    "Write a function `def l2_reg_create_layer(prev, n, activation, lambtha):` that creates a tensorflow layer that includes L2 regularization:\n",
    "\n",
    "`prev` is a tensor containing the output of the previous layer\n",
    "`n` is the number of nodes the new layer should contain\n",
    "`activation` is the activation function that should be used on the layer\n",
    "`lambtha` is the L2 regularization parameter\n",
    "Returns: the output of the new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64614a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that creates a tensorflow layer that includes\n",
    "L2 regularization\n",
    "'''\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "def l2_reg_create_layer(prev, n, activation, lambtha):\n",
    "    '''\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prev : TYPE tensor\n",
    "        DESCRIPTION. Is a tensor containing the output of the\n",
    "        previous layer\n",
    "    n : TYPE int\n",
    "        DESCRIPTION. Number of nodes\n",
    "    activation : TYPE tensor\n",
    "        DESCRIPTION. Type of activation to be used in the layer \n",
    "    lambtha : TYPE float\n",
    "        DESCRIPTION. L2 regularization parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The output of the new layer.\n",
    "\n",
    "    '''\n",
    "    init = tf.keras.initializers.VarianceScaling(scale=2.0,\n",
    "                                                 mode=(\"fan_avg\"))\n",
    "    l2 = tf.keras.regularizers.L2(lambtha)\n",
    "    layer = tf.layers.Dense(n,\n",
    "                            activation=activation,\n",
    "                            kernel_initializer=init,\n",
    "                            kernel_regularizer=l2\n",
    "                            )(prev)\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51cbc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.440594  49.658012   6.0545483 41.180496   2.5326564]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "# l2_reg_cost = __import__('2-l2_reg_cost').l2_reg_cost\n",
    "# l2_reg_create_layer = __import__('3-l2_reg_create_layer').l2_reg_create_layer\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((m, classes))\n",
    "    one_hot[np.arange(m), Y] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1))\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    tf.set_random_seed(0)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    h1 = l2_reg_create_layer(x, 256, tf.nn.tanh, 0.05)\n",
    "    y_pred = l2_reg_create_layer(x, 10, None, 0.)\n",
    "    cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "    l2_cost = l2_reg_cost(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(l2_cost, feed_dict={x: X_train, y: Y_train_oh}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe3375",
   "metadata": {},
   "source": [
    "# 4. Forward Propagation with Dropout\n",
    "Write a function `def dropout_forward_prop(X, weights, L, keep_prob):` that conducts forward propagation using Dropout:\n",
    "\n",
    "* `X` is a numpy.ndarray of shape `(nx, m)` containing the input data for the network\n",
    "    * `nx` is the number of input features\n",
    "    * `m` is the number of data points\n",
    "* `weights` is a dictionary of the weights and biases of the neural network\n",
    "* `L` the number of layers in the network\n",
    "* `keep_prob` is the probability that a node will be kept\n",
    "* All layers except the last should use the `tanh` activation function\n",
    "* The last layer should use the `softmax` activation function\n",
    "* Returns: a dictionary containing the outputs of each layer and the dropout mask used on each layer (see example for format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c913ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that conducts forward propagation usig dropout\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dropout_forward_prop(X, weights, L, keep_prob):\n",
    "    '''\n",
    "    Function  that conducts forward propagation using Dropout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : TYPE numpy.ndarray\n",
    "        DESCRIPTION. X is a numpy.ndarray of shape (nx, m) containing\n",
    "        the input data for the network. nx - number of input features,\n",
    "        m - number of data points\n",
    "    weights : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the weights and biases of NN\n",
    "    L : TYPE int\n",
    "        DESCRIPTION. Number of layers\n",
    "    keep_prob : TYPE float\n",
    "        DESCRIPTION. probability that the node will be kept\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the outputs of each layer and the dropout\n",
    "    mask used on each layer (see example for format).\n",
    "\n",
    "    '''\n",
    "    cache = {}\n",
    "    cache['A0'] = X\n",
    "    for i in range(1, L + 1):\n",
    "        w = weights['W' + str(i)]\n",
    "        b = weights['b' + str(i)]\n",
    "        a0 = cache['A' + str(i - 1)]\n",
    "        Z = np.matmul(w, a0) + b\n",
    "        if i == L:\n",
    "            num = np.exp(Z)\n",
    "            A = num / np.sum(num, axis=0, keepdims=True)\n",
    "            cache['A' + str(i)] = A\n",
    "        else:\n",
    "            A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "            D = np.random.rand(A.shape[0], A.shape[1])\n",
    "            D = np.where(D < keep_prob, 1, 0)\n",
    "            A *= D\n",
    "            A /= keep_prob\n",
    "            cache['A' + str(i)] = A\n",
    "            cache['D' + str(i)] = D\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42240b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "A1 [[-1.24999999 -1.25       -1.24999945 ... -1.25       -1.25\n",
      "  -1.25      ]\n",
      " [ 1.25        1.24999777  1.25       ...  0.37738875  1.24999717\n",
      "  -1.24999889]\n",
      " [ 0.19383179 -0.80653094 -1.24950714 ...  1.24253535  1.08653948\n",
      "  -1.20190135]\n",
      " ...\n",
      " [-1.25       -1.25        0.         ... -0.         -1.25\n",
      "  -1.24999852]\n",
      " [-1.0858595  -1.25        0.         ...  1.24972487 -0.88878698\n",
      "  -1.24999933]\n",
      " [ 1.25        1.24999648  0.2057473  ...  0.          1.23194191\n",
      "  -1.24908257]]\n",
      "A2 [[-1.25        0.          1.24985922 ... -1.25        0.\n",
      "   1.24996854]\n",
      " [-0.         -0.         -0.         ... -1.24996232 -0.70684864\n",
      "   1.25      ]\n",
      " [-1.25        0.          0.18486152 ... -1.24999999 -1.25\n",
      "  -1.24999989]\n",
      " ...\n",
      " [ 1.2404131   1.25        1.25       ...  1.1670038   1.25\n",
      "  -0.        ]\n",
      " [ 1.25        1.25       -1.24998041 ...  1.2400913  -1.25\n",
      "   1.23620006]\n",
      " [ 0.93426582  1.25        1.25       ...  1.24999867 -1.25\n",
      "  -0.        ]]\n",
      "A3 [[9.13222086e-07 1.53352996e-09 4.02988574e-13 ... 2.93685964e-04\n",
      "  2.21615443e-11 7.95945899e-04]\n",
      " [4.10709405e-16 4.27810333e-11 7.38725096e-07 ... 2.05423847e-17\n",
      "  2.66482686e-09 1.74341031e-12]\n",
      " [9.82953561e-01 9.88655425e-01 9.73580864e-01 ... 1.14493065e-03\n",
      "  9.28074126e-10 1.92423905e-13]\n",
      " ...\n",
      " [3.03047424e-04 1.11981605e-02 4.72284535e-05 ... 1.25781567e-20\n",
      "  9.57462819e-01 3.33328605e-13]\n",
      " [3.20689297e-11 7.42324257e-08 5.62529910e-19 ... 2.05682936e-16\n",
      "  1.07622653e-12 1.41200115e-02]\n",
      " [5.06603174e-06 8.50852457e-11 5.51467429e-10 ... 9.98493133e-01\n",
      "  1.97896353e-14 2.38078250e-05]]\n",
      "D1 [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 0 ... 0 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [1 1 1 ... 0 1 1]]\n",
      "D2 [[1 0 1 ... 1 0 1]\n",
      " [0 0 0 ... 1 1 1]\n",
      " [1 0 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "# dropout_forward_prop = __import__('4-dropout_forward_prop').dropout_forward_prop\n",
    "\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "    cache = dropout_forward_prop(X_train, weights, 3, 0.8)\n",
    "    for k, v in sorted(cache.items()):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783371c",
   "metadata": {},
   "source": [
    "# 5. Gradient Descent with Dropout\n",
    "Write a function `def dropout_gradient_descent(Y, weights, cache, alpha, keep_prob, L):` that updates the weights of a neural network with Dropout regularization using gradient descent:\n",
    "\n",
    "* `Y` is a one-hot `numpy.ndarray` of shape `(classes, m)` that contains the correct labels for the data\n",
    "    * `classes` is the number of classes\n",
    "* `m` is the number of data points\n",
    "* `weights` is a dictionary of the weights and biases of the neural network\n",
    "* `cache` is a dictionary of the outputs and dropout masks of each layer of the neural network\n",
    "* `alpha` is the learning rate\n",
    "* `keep_prob` is the probability that a node will be kept\n",
    "* `L` is the number of layers of the network\n",
    "* All layers use the `tanh` activation function except the last, which uses the `softmax` activation function\n",
    "The weights of the network should be updated in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ed7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that  updates the weights of a neural network with\n",
    "Dropout regularization using gradient descent\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dropout_gradient_descent(Y, weights, cache, alpha, keep_prob, L):\n",
    "    '''\n",
    "    Function  that updates the weights of a neural network with\n",
    "    Dropout regularization using gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : TYPE numpy.ndarray\n",
    "        DESCRIPTION. Y is a one-hot numpy.ndarray of shape (classes, m)\n",
    "        that contains the correct labels for the data\n",
    "    weights : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the weights and biases of the neural network\n",
    "    cache : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the outputs and dropout masks of\n",
    "        each layer of the neural network\n",
    "    alpha : TYPE float\n",
    "        DESCRIPTION. Learning rate\n",
    "    keep_prob : TYPE float\n",
    "        DESCRIPTION. Probabily that a node will be kept\n",
    "    L : TYPE int\n",
    "        DESCRIPTION. Number of layers of DNN\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        w = weights['W' + str(i)]\n",
    "        b = weights['b' + str(i)]\n",
    "        a0 = cache['A' + str(i - 1)]\n",
    "        a = cache['A' + str(i)]\n",
    "        if i == L:\n",
    "            dz = a - Y\n",
    "            W = w\n",
    "        else:\n",
    "            d = cache['D' + str(i)]\n",
    "            da = 1 - (a * a)\n",
    "            dz = np.matmul(W.T, dz)\n",
    "            dz = dz * da * d\n",
    "            dz = dz / keep_prob\n",
    "            W = w\n",
    "        dw = np.matmul(a0, dz.T) / m\n",
    "        db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "        weights['W' + str(i)] = w - alpha * dw.T\n",
    "        weights['b' + str(i)] = b - alpha * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02c463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.9282086  -0.71324613 -1.33191318 ... -2.14202626 -0.07737407\n",
      "   0.99832167]\n",
      " [-0.0237149  -0.18364778  0.08337452 ... -0.06093055 -0.03924408\n",
      "  -2.17625294]\n",
      " [-0.16181888  0.49237435 -0.47196279 ...  0.97504077  0.16272698\n",
      "   0.56159916]\n",
      " ...\n",
      " [ 0.39842474 -0.09870005  1.32173992 ... -0.33210834  0.66215988\n",
      "   0.87211421]\n",
      " [ 0.15767221  0.42236212  1.004765   ...  0.69883284  0.70857088\n",
      "  -0.44427252]\n",
      " [ 2.68588811 -0.60351958 -1.0759598  ... -1.2437044   0.69462324\n",
      "   1.00090403]]\n",
      "[[-1.92044686 -0.71894673 -1.32811693 ... -2.14071955 -0.07158198\n",
      "   0.98206832]\n",
      " [-0.03706116 -0.17088483  0.07798748 ... -0.07245569 -0.0491215\n",
      "  -2.16245276]\n",
      " [-0.17198668  0.49842244 -0.47369328 ...  0.96880194  0.15497217\n",
      "   0.5693131 ]\n",
      " ...\n",
      " [ 0.41997262 -0.11452751  1.32873227 ... -0.31312321  0.67162237\n",
      "   0.85928296]\n",
      " [ 0.13702353  0.44237056  1.00139188 ...  0.68128208  0.69020934\n",
      "  -0.43055442]\n",
      " [ 2.66514017 -0.59204122 -1.08943163 ... -1.26238074  0.69280683\n",
      "   1.02353101]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "# dropout_forward_prop = __import__('4-dropout_forward_prop').dropout_forward_prop\n",
    "# dropout_gradient_descent = __import__('5-dropout_gradient_descent').dropout_gradient_descent\n",
    "\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "    cache = dropout_forward_prop(X_train, weights, 3, 0.8)\n",
    "    print(weights['W2'])\n",
    "    dropout_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.8, 3)\n",
    "    print(weights['W2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec47975",
   "metadata": {},
   "source": [
    "# 6. Create a Layer with Dropout\n",
    "Write a function `def dropout_create_layer(prev, n, activation, keep_prob):` that creates a layer of a neural network using dropout:\n",
    "\n",
    "* `prev` is a tensor containing the output of the previous layer\n",
    "* `n` is the number of nodes the new layer should contain\n",
    "* `activation` is the activation function that should be used on the layer\n",
    "* `keep_prob` is the probability that a node will be kept\n",
    "* Returns: the output of the new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1875ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''Moudulus that creates a layer of NN using droput'''\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "def dropout_create_layer(prev, n, activation, keep_prob):\n",
    "    '''\n",
    "    Function that creates a layer of a neural network using dropout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prev : TYPE tensor\n",
    "        DESCRIPTION. Output of the previus layer\n",
    "    n : TYPE int\n",
    "        DESCRIPTION. Number of nodes\n",
    "    activation : TYPE str\n",
    "        DESCRIPTION. Activation functions that should be used in the layer\n",
    "    keep_prob : TYPE float\n",
    "        DESCRIPTION. Probability of the node to be kept\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The output of the new layer.\n",
    "\n",
    "    '''\n",
    "    init = tf.keras.initializers.VarianceScaling(scale=2.0,\n",
    "                                                 mode=(\"fan_avg\"))\n",
    "    drop = tf.layers.Dropout(rate=keep_prob)\n",
    "    layer = tf.layers.Dense(n,\n",
    "                            activation=activation,\n",
    "                            kernel_initializer=init,\n",
    "                            kernel_regularizer=drop)(prev)\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3eb1468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.         -0.99999267 ...  1.         -1.\n",
      "  -1.        ]\n",
      " [ 1.         -1.          1.         ...  1.         -1.\n",
      "  -1.        ]\n",
      " [ 1.         -1.          1.         ...  1.         -1.\n",
      "  -1.        ]\n",
      " ...\n",
      " [ 1.         -1.          1.         ... -0.9999962  -1.\n",
      "  -1.        ]\n",
      " [ 1.         -1.         -1.         ... -1.         -1.\n",
      "   1.        ]\n",
      " [ 1.         -1.          1.         ...  1.         -1.\n",
      "  -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "# dropout_create_layer = __import__('6-dropout_create_layer').dropout_create_layer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.set_random_seed(0)\n",
    "    np.random.seed(0)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    X = np.random.randint(0, 256, size=(10, 784))\n",
    "    a = dropout_create_layer(x, 256, tf.nn.tanh, 0.8)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(a, feed_dict={x: X}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273ff44",
   "metadata": {},
   "source": [
    "# 7. Early Stopping\n",
    "Write the function `def early_stopping(cost, opt_cost, threshold, patience, count):` that determines if you should stop gradient descent early:\n",
    "\n",
    "* Early stopping should occur when the validation cost of the network has not decreased relative to the optimal validation cost by more than the threshold over a specific patience count\n",
    "* `cost` is the current validation cost of the neural network*\n",
    "* `opt_cost` is the lowest recorded validation cost of the neural network\n",
    "* `threshold` is the threshold used for early stopping\n",
    "* `patience` is the patience count used for early stopping\n",
    "* `count` is the count of how long the threshold has not been met\n",
    "* Returns: a boolean of whether the network should be stopped early, followed by the updated count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "becf31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that determines if should be stopped gradient descent early\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def early_stopping(cost, opt_cost, threshold, patience, count):\n",
    "    '''\n",
    "    Function that determines if you should stop gradient descent early\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cost : TYPE float\n",
    "        DESCRIPTION. Current validation cost of the NN\n",
    "    opt_cost : TYPE float\n",
    "        DESCRIPTION. Lowest recorded validation cost of NN\n",
    "    threshold : TYPE\n",
    "        DESCRIPTION. Threshold used for early stopping\n",
    "    patience : TYPE int\n",
    "        DESCRIPTION. Patience count used for early stopping\n",
    "    count : TYPE int\n",
    "        DESCRIPTION. Count of how long the threshold has not been met\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A boolean of whether the network should be stopped early,\n",
    "    followed by the updated count.\n",
    "\n",
    "    '''\n",
    "    if opt_cost - cost <= threshold:\n",
    "        count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "    if count == patience:\n",
    "        return (True, count)\n",
    "    else:\n",
    "        return (False, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff722be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, 0)\n",
      "(False, 3)\n",
      "(False, 9)\n",
      "(True, 15)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# early_stopping = __import__('7-early_stopping').early_stopping\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(early_stopping(1.0, 1.9, 0.5, 15, 5))\n",
    "    print(early_stopping(1.1, 1.5, 0.5, 15, 2))\n",
    "    print(early_stopping(1.0, 1.5, 0.5, 15, 8))\n",
    "    print(early_stopping(1.0, 1.5, 0.5, 15, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15339d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

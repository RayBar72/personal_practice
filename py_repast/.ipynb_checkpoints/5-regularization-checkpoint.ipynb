{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47ac6f7",
   "metadata": {},
   "source": [
    "# 0. L2 Regularization Cost\n",
    "Write a function `def l2_reg_cost(cost, lambtha, weights, L, m):` that calculates the cost of a neural network with L2 regularization:\n",
    "\n",
    "- `cost` is the cost of the network without L2 regularization\n",
    "- `lambtha` is the regularization parameter\n",
    "- `weights` is a dictionary of the weights and biases (`numpy.ndarrays`) of the neural network\n",
    "- `L` is the number of layers in the neural network\n",
    "- `m` is the number of data points used\n",
    "- Returns: the cost of the network accounting for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009cc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def l2_reg_cost(cost, lambtha, weights, L, m):\n",
    "    W = [weights['W' + str(i + 1)] for i in range(L)]\n",
    "    W = [np.linalg.norm(w) ** 2 for w in W]\n",
    "    c = cost + lambtha * sum(W) / (2 * m)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868fb2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41842822]\n",
      "[12.11229237]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "\n",
    "    cost = np.abs(np.random.randn(1))\n",
    "\n",
    "    print(cost)\n",
    "    cost = l2_reg_cost(cost, 0.1, weights, 3, 1000)\n",
    "    print(cost)\n",
    "#     W = [weights['W' + str(i + 1)] for i in range(3)]\n",
    "#     print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d34c4",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent with L2 Regularization\n",
    "Write a function `def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):` that updates the weights and biases of a neural network using gradient descent with L2 regularization:\n",
    "\n",
    "* `Y` is a one-hot `numpy.ndarray` of shape (`classes, m`) that contains the correct labels for the data\n",
    "    * `classes` is the number of classes\n",
    "    * `m` is the number of data points\n",
    "* `weights` is a dictionary of the weights and biases of the neural network\n",
    "* `cache` is a dictionary of the outputs of each layer of the neural network\n",
    "* `alpha` is the learning rate\n",
    "* `lambtha` is the L2 regularization parameter\n",
    "* `L` is the number of layers of the network\n",
    "* The neural network uses `tanh` activations on each layer except the last, which uses a `softmax` activation\n",
    "* The weights and biases of the network should be updated in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107e9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that updates the weights and biases of a neural\n",
    "network using gradient descent with L2 regularization\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):\n",
    "    '''\n",
    "    Function that updates the weights and biases of a neural\n",
    "    network using gradient descent with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : TYPE numpy.ndarray\n",
    "        DESCRIPTION. Y is a one-hot numpy.ndarray of shape (classes, m)\n",
    "        that contains the correct labels for the data\n",
    "    weights : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the weights and biases of the\n",
    "        neural network\n",
    "    cache : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the outputs of each layer of\n",
    "        the neural network\n",
    "    alpha : TYPE float\n",
    "        DESCRIPTION. Learning rate\n",
    "    lambtha : TYPE float\n",
    "        DESCRIPTION. L2 regularization parameter\n",
    "    L : TYPE int\n",
    "        DESCRIPTION. layers of the network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    for i in reversed(range(L)):\n",
    "        w = 'W' + str(i + 1)\n",
    "        b = 'b' + str(i + 1)\n",
    "        a = 'A' + str(i + 1)\n",
    "        a_0 = 'A' + str(i)\n",
    "        A = cache[a]\n",
    "        A_dw = cache[a_0]\n",
    "        if i == L - 1:\n",
    "            dz = A - Y\n",
    "            W = weights[w]\n",
    "        else:\n",
    "            da = 1 - (A * A)\n",
    "            dz = np.matmul(W.T, dz)\n",
    "            dz = dz * da\n",
    "            W = weights[w]\n",
    "        dw = np.matmul(A_dw, dz.T) / m\n",
    "        db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "        weights[w] = weights[w] - alpha * (dw.T + (lambtha / m * weights[w]))\n",
    "        weights[b] = weights[b] - alpha * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4709a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719\n",
      "  -1.34149673]\n",
      " [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133\n",
      "   0.07912172]\n",
      " [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445\n",
      "  -1.07836109]\n",
      " ...\n",
      " [-0.60467085  0.54751161 -1.23317415 ...  0.82895532  1.44161136\n",
      "   0.18972404]\n",
      " [-0.41044606  0.85719512  0.71789835 ... -0.73954771  0.5074628\n",
      "   1.23022874]\n",
      " [ 0.43129249  0.60767018 -0.07749988 ... -0.26611561  2.52287972\n",
      "   0.73131543]]\n",
      "[[ 1.76405199  0.40015713  0.97873779 ...  0.52130364  0.61192707\n",
      "  -1.34149646]\n",
      " [ 0.47689827  0.14844955  0.52904513 ...  0.09600419 -0.04511329\n",
      "   0.07912171]\n",
      " [ 0.85053051 -0.83912402 -1.01177388 ... -0.07223874  0.31112438\n",
      "  -1.07836088]\n",
      " ...\n",
      " [-0.60467073  0.5475115  -1.2331739  ...  0.82895516  1.44161107\n",
      "   0.189724  ]\n",
      " [-0.41044598  0.85719495  0.71789821 ... -0.73954756  0.5074627\n",
      "   1.2302285 ]\n",
      " [ 0.4312924   0.60767006 -0.07749987 ... -0.26611556  2.52287922\n",
      "   0.73131529]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "# l2_reg_gradient_descent = __import__('1-l2_reg_gradient_descent').l2_reg_gradient_descent\n",
    "\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "    cache = {}\n",
    "    cache['A0'] = X_train\n",
    "    cache['A1'] = np.tanh(np.matmul(weights['W1'], cache['A0']) + weights['b1'])\n",
    "    cache['A2'] = np.tanh(np.matmul(weights['W2'], cache['A1']) + weights['b2'])\n",
    "    Z3 = np.matmul(weights['W3'], cache['A2']) + weights['b3']\n",
    "    cache['A3'] = np.exp(Z3) / np.sum(np.exp(Z3), axis=0)\n",
    "    print(weights['W1'])\n",
    "    l2_reg_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.1, 3)\n",
    "    print(weights['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa182c9",
   "metadata": {},
   "source": [
    "# 2. L2 Regularization Cost\n",
    "Write the function `def l2_reg_cost(cost):` that calculates the cost of a neural network with L2 regularization:\n",
    "\n",
    "* `cost` is a tensor containing the cost of the network without L2 regularization\n",
    "* Returns: a tensor containing the cost of the network accounting for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd3cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Moudulus that calculates the cost of a NN with l2 regularization\n",
    "'''\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "def l2_reg_cost(cost):\n",
    "    '''\n",
    "    Function that calculates the cost of a neural network\n",
    "    with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cost : TYPE tensor\n",
    "        DESCRIPTION. Tensor containing the cost\n",
    "        of the network without L2 regularization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tensor containing the cost of the network accounting for\n",
    "    L2 regularization.\n",
    "\n",
    "    '''\n",
    "    cost = cost + tf.losses.get_regularization_loss()\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1746e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Moudulus that calculates the cost of a NN with l2 regularization\n",
    "'''\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "def l2_reg_cost(cost):\n",
    "    '''\n",
    "    Function that calculates the cost of a neural network\n",
    "    with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cost : TYPE tensor\n",
    "        DESCRIPTION. Tensor containing the cost\n",
    "        of the network without L2 regularization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tensor containing the cost of the network accounting for\n",
    "    L2 regularization.\n",
    "\n",
    "    '''\n",
    "    return cost + tf.losses.get_regularization_losses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49dfc7",
   "metadata": {},
   "source": [
    "## Ojo\n",
    "* En los recursos leidos se tomaba get_regularization_loss. Esto genera un vector 1x1\n",
    "* En el video de resoluciÃ³n se utiliza get_regularization_loss**es** esto arroja un vector de 1x3\n",
    "* Si se corre varias veces el programa, la memoria va aumentando el tamaÃ±o del vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10c3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.430164 49.647583  6.044121]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "# l2_reg_cost = __import__('2-l2_reg_cost').l2_reg_cost\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    oh = np.zeros((classes, m))\n",
    "    oh[Y, np.arange(m)] = 1\n",
    "    return oh\n",
    "\n",
    "np.random.seed(4)\n",
    "m = np.random.randint(1000, 2000)\n",
    "c = 10\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "\n",
    "X = lib['X_train'][:m].reshape((m, -1))\n",
    "Y = one_hot(lib['Y_train'][:m], c).T\n",
    "\n",
    "n0 = X.shape[1]\n",
    "n1, n2 = np.random.randint(10, 1000, 2)\n",
    "\n",
    "lam = 0.09\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, n0))\n",
    "y = tf.placeholder(tf.float32, (None, c))\n",
    "\n",
    "a1 = tf.layers.Dense(n1, activation=tf.nn.tanh, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode=(\"fan_avg\")), kernel_regularizer=tf.keras.regularizers.L2(lam))(x)\n",
    "a2 = tf.layers.Dense(n2, activation=tf.nn.sigmoid, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode=(\"fan_avg\")), kernel_regularizer=tf.keras.regularizers.L2(lam))(a1)\n",
    "y_pred = tf.layers.Dense(c, activation=None, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode=(\"fan_avg\")), kernel_regularizer=tf.keras.regularizers.L2(lam))(a2)\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "\n",
    "l2_cost = l2_reg_cost(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(l2_cost, feed_dict={x: X, y: Y}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060e3e3",
   "metadata": {},
   "source": [
    "# 3. Create a Layer with L2 Regularization\n",
    "Write a function `def l2_reg_create_layer(prev, n, activation, lambtha):` that creates a tensorflow layer that includes L2 regularization:\n",
    "\n",
    "`prev` is a tensor containing the output of the previous layer\n",
    "`n` is the number of nodes the new layer should contain\n",
    "`activation` is the activation function that should be used on the layer\n",
    "`lambtha` is the L2 regularization parameter\n",
    "Returns: the output of the new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64614a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that creates a tensorflow layer that includes\n",
    "L2 regularization\n",
    "'''\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "def l2_reg_create_layer(prev, n, activation, lambtha):\n",
    "    '''\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prev : TYPE tensor\n",
    "        DESCRIPTION. Is a tensor containing the output of the\n",
    "        previous layer\n",
    "    n : TYPE int\n",
    "        DESCRIPTION. Number of nodes\n",
    "    activation : TYPE tensor\n",
    "        DESCRIPTION. Type of activation to be used in the layer \n",
    "    lambtha : TYPE float\n",
    "        DESCRIPTION. L2 regularization parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The output of the new layer.\n",
    "\n",
    "    '''\n",
    "    init = tf.keras.initializers.VarianceScaling(scale=2.0,\n",
    "                                                 mode=(\"fan_avg\"))\n",
    "    l2 = tf.keras.regularizers.L2(lambtha)\n",
    "    layer = tf.layers.Dense(n,\n",
    "                            activation=activation,\n",
    "                            kernel_initializer=init,\n",
    "                            kernel_regularizer=l2\n",
    "                            )(prev)\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51cbc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.440594  49.658012   6.0545483 41.180496   2.5326564]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "# l2_reg_cost = __import__('2-l2_reg_cost').l2_reg_cost\n",
    "# l2_reg_create_layer = __import__('3-l2_reg_create_layer').l2_reg_create_layer\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((m, classes))\n",
    "    one_hot[np.arange(m), Y] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1))\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    tf.set_random_seed(0)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    h1 = l2_reg_create_layer(x, 256, tf.nn.tanh, 0.05)\n",
    "    y_pred = l2_reg_create_layer(x, 10, None, 0.)\n",
    "    cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "    l2_cost = l2_reg_cost(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(l2_cost, feed_dict={x: X_train, y: Y_train_oh}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe3375",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f65227",
   "metadata": {},
   "source": [
    "# 0. Sequential\n",
    "Write a function `def build_model(nx, layers, activations, lambtha, keep_prob):` that builds a neural network with the Keras library:\n",
    "\n",
    "* `nx` is the number of input features to the network\n",
    "* `layers` is a `list` containing the number of nodes in each layer of the network\n",
    "* `activations` is a `list` containing the activation functions used for each layer of the network\n",
    "* `lambtha` is the L2 regularization parameter\n",
    "* `keep_prob` is the probability that a node will be kept for dropout\n",
    "* You are not allowed to use the Input class\n",
    "* Returns: the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d910470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that creates a neural network with\n",
    "keras library\n",
    "'''\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    '''\n",
    "    Function that builds a neural network with the Keras library\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nx : TYPE int\n",
    "        DESCRIPTION. Number of input features to the network\n",
    "    layers : TYPE list\n",
    "        DESCRIPTION. List containing the number of nodes in each\n",
    "        layer of the network\n",
    "    activations : TYPE list\n",
    "        DESCRIPTION. List containing the activation functions used for\n",
    "        each layer of the network\n",
    "    lambtha : TYPE float\n",
    "        DESCRIPTION. Is the L2 regularization parÃ¡meter\n",
    "    keep_prob : TYPE float\n",
    "        DESCRIPTION. probability that a node will be kept of dropout\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The keras model.\n",
    "\n",
    "    '''\n",
    "    model = K.models.Sequential()\n",
    "    la2 = K.regularizers.l2(lambtha)\n",
    "    i = 0\n",
    "    for layer, activa in zip(layers, activations):\n",
    "        if i == 0:\n",
    "            model.add(K.layers.Dense(layer,\n",
    "                                     input_dim=nx,\n",
    "                                     activation=activa,\n",
    "                                     kernel_regularizer=la2))\n",
    "            i += 1\n",
    "        else:\n",
    "            drop = K.layers.Dropout(rate=1 - keep_prob)\n",
    "            model.add(drop)\n",
    "            model.add(K.layers.Dense(layer,\n",
    "                                     activation=activa,\n",
    "                                     kernel_regularizer=la2))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "# build_model = __import__('0-sequential').build_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)\n",
    "    print(network.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0bd059",
   "metadata": {},
   "source": [
    "# 1. Input\n",
    "Write a function `def build_model(nx, layers, activations, lambtha, keep_prob):` that builds a neural network with the Keras library:\n",
    "\n",
    "* `nx` is the number of input features to the network\n",
    "* `layers` is a list containing the number of nodes in each layer of the network\n",
    "* `activations` is a list containing the activation functions used for each layer of the network\n",
    "* `lambtha` is the L2 regularization parameter\n",
    "* `keep_prob` is the probability that a node will be kept for dropout\n",
    "* You are not allowed to use the `Sequential` class\n",
    "* Returns: the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f32947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that builds a NN with keares\n",
    "Not using Sequential class\n",
    "'''\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    '''\n",
    "    Function that builds a neural network with the Keras library\n",
    "    It is not allowed to use Sequential class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nx : TYPE. int\n",
    "        DESCRIPTION. Number of input features\n",
    "    layers : TYPE. list\n",
    "        DESCRIPTION. List containing number of nodes in each layer\n",
    "    activations : TYPE. list\n",
    "        DESCRIPTION. List containing the activation function in each layer\n",
    "    lambtha : TYPE. float\n",
    "        DESCRIPTION. Regularization parameter\n",
    "    keep_prob : TYPE. float\n",
    "        DESCRIPTION. Probability that a node will be kept for dropout\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Keras model.\n",
    "\n",
    "    '''\n",
    "    Inputs = K.Input(shape=(nx,))\n",
    "    l2 = K.regularizers.l2(lambtha)\n",
    "    i = 0\n",
    "    for lay, act in zip(layers, activations):\n",
    "        if i == 0:\n",
    "            L = K.layers.Dense(lay,\n",
    "                               activation=act,\n",
    "                               kernel_regularizer=l2)(Inputs)\n",
    "            i += 1\n",
    "        else:\n",
    "            L = K.layers.Dropout(1 - keep_prob)(L)\n",
    "            L = K.layers.Dense(lay,\n",
    "                               activation=act,\n",
    "                               kernel_regularizer=l2)(L)\n",
    "    model = K.Model(inputs=Inputs, outputs=L)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "# build_model = __import__('1-input').build_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673958c",
   "metadata": {},
   "source": [
    "# 2. Optimize\n",
    "Write a function `def optimize_model(network, alpha, beta1, beta2):` that sets up Adam optimization for a keras model with categorical crossentropy loss and accuracy metrics:\n",
    "\n",
    "* `network` is the model to optimize\n",
    "* `alpha` is the learning rate\n",
    "* `beta1` is the first Adam optimization parameter\n",
    "* `beta2` is the second Adam optimization parameter\n",
    "* Returns: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8602da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that sets up Adam optimization for keras model\n",
    "with categoriacl crossentropy loss and accuracy metrics\n",
    "'''\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    '''\n",
    "    Function that sets up Adam optimization for a keras model with\n",
    "    categorical crossentropy loss and accuracy metrics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : TYPE tensor\n",
    "        DESCRIPTION. Model to optimize\n",
    "    alpha : TYPE float\n",
    "        DESCRIPTION. Learning rate\n",
    "    beta1 : TYPE Float\n",
    "        DESCRIPTION. First Adam optimization parameter\n",
    "    beta2 : TYPE float\n",
    "        DESCRIPTION. Second Adam optimization parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    opt = K.optimizers.Adam(learning_rate=alpha,\n",
    "                            beta_1=beta1,\n",
    "                            beta_2=beta2)\n",
    "    network.compile(optimizer=opt,\n",
    "                    loss=K.losses.CategoricalCrossentropy(),\n",
    "                    metrics=['accuracy'])\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11169d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# build_model = __import__('1-input').build_model\n",
    "# optimize_model = __import__('2-optimize').optimize_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    optimize_model(model, 0.01, 0.99, 0.9)\n",
    "    print(model.loss)\n",
    "    opt = model.optimizer\n",
    "    print(opt.__class__)\n",
    "    print(tuple(map(lambda x: x.numpy(),(opt.lr, opt.beta_1, opt.beta_2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3bab59",
   "metadata": {},
   "source": [
    "# 3. One Hot\n",
    "Write a function `def one_hot(labels, classes=None):` that converts a label vector into a one-hot matrix:\n",
    "\n",
    "* The last dimension of the one-hot matrix must be the number of classes\n",
    "* Returns: the one-hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9c13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that converts a label vector in to\n",
    "one-hot matrix\n",
    "'''\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def one_hot(labels, classes=None):\n",
    "    '''\n",
    "    Function that converts a label vector into a one-hot matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : TYPE vector\n",
    "        DESCRIPTION. Vector to be converted in one-hot matrix\n",
    "    classes : TYPE, optional\n",
    "        DESCRIPTION. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None. The last dimension must be number of classes\n",
    "\n",
    "    '''\n",
    "    return K.utils.to_categorical(labels, classes, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd711de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "# one_hot = __import__('3-one_hot').one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "    print(labels)\n",
    "    print(one_hot(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4f109",
   "metadata": {},
   "source": [
    "# 4. Train\n",
    "Write a function  `def train_model(network, data, labels, batch_size, epochs, verbose=True, shuffle=False):` that trains a model using mini-batch gradient descent:\n",
    "\n",
    "* `network` is the model to train\n",
    "* `data` is a `numpy.ndarray` of shape (`m, nx`) containing the input data\n",
    "* `labels` is a one-hot `numpy.ndarray` of shape (`m, classes`) containing the labels of data\n",
    "* `batch_size` is the size of the batch used for mini-batch gradient descent\n",
    "* `epochs` is the number of passes through data for mini-batch gradient descent\n",
    "* `verbose` is a `boolean` that determines if output should be printed during training\n",
    "* `shuffle` is a `boolean` that determines whether to shuffle the batches every epoch. Normally, it is a good idea to shuffle, but for reproducibility, we have chosen to set the default to False.\n",
    "* Returns: the History object generated after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that trins a model using mini-batch gradien descent\n",
    "'''\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                verbose=True, shuffle=False):\n",
    "    '''\n",
    "    Function that trains a model using mini-batch gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : TYPE model\n",
    "        DESCRIPTION. Model to be train\n",
    "    data : TYPE numpy.ndarray\n",
    "        DESCRIPTION. data is a numpy.ndarray of shape (m, nx) containing\n",
    "        the input data\n",
    "    labels : TYPE numpy.ndarray\n",
    "        DESCRIPTION. (m, classes) containing the labels of data\n",
    "    batch_size : TYPE int\n",
    "        DESCRIPTION. Batch size used for mini-batch gradient descent\n",
    "    epochs : TYPE int\n",
    "        DESCRIPTION. Number of passes through data for mini-batch g.d.\n",
    "    verbose : TYPE, optional\n",
    "        DESCRIPTION. The default is True. Determines if output should be\n",
    "        printed during the training\n",
    "    shuffle : TYPE, optional\n",
    "        DESCRIPTION. The default is False. Determines if shuffle the batches\n",
    "        every epoch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    History object generated after training model.\n",
    "\n",
    "    '''\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "# Imports\n",
    "# build_model = __import__('1-input').build_model\n",
    "# optimize_model = __import__('2-optimize').optimize_model\n",
    "# one_hot = __import__('3-one_hot').one_hot\n",
    "# train_model = __import__('4-train').train_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bcbba",
   "metadata": {},
   "source": [
    "# 5. Validate\n",
    "Based on `4-train.py`, update the function `def train_model(network, data, labels, batch_size, epochs, validation_data=None, verbose=True, shuffle=False):` to also analyze validaiton data:\n",
    "\n",
    "* `validation_data` is the data to validate the model with, if not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc615af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that trins a model using mini-batch gradien descent\n",
    "'''\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, verbose=True, shuffle=False):\n",
    "    '''\n",
    "    Based on 4-train.py, update the function train_model\n",
    "    to also analyze validaiton data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : TYPE model\n",
    "        DESCRIPTION. Model to be train\n",
    "    data : TYPE numpy.ndarray\n",
    "        DESCRIPTION. data is a numpy.ndarray of shape (m, nx) containing\n",
    "        the input data\n",
    "    labels : TYPE numpy.ndarray\n",
    "        DESCRIPTION. (m, classes) containing the labels of data\n",
    "    batch_size : TYPE int\n",
    "        DESCRIPTION. Batch size used for mini-batch gradient descent\n",
    "    epochs : TYPE int\n",
    "        DESCRIPTION. Number of passes through data for mini-batch g.d.\n",
    "    validation_data : TYPE, optional\n",
    "        DESCRIPTION. Dato to validate the model.\n",
    "    verbose : TYPE, optional\n",
    "        DESCRIPTION. The default is True.\n",
    "    shuffle : TYPE, optional\n",
    "        DESCRIPTION. The default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    if validation_data:\n",
    "        validation_data = validation_data\n",
    "    else:\n",
    "        validation_data = None\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       validation_data=validation_data,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1346d74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.3253 - accuracy: 0.9208 - val_loss: 0.1879 - val_accuracy: 0.9627\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1746 - accuracy: 0.9657 - val_loss: 0.1589 - val_accuracy: 0.9694\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1420 - accuracy: 0.9755 - val_loss: 0.1558 - val_accuracy: 0.9712\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1259 - accuracy: 0.9803 - val_loss: 0.1594 - val_accuracy: 0.9722\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1148 - accuracy: 0.9832 - val_loss: 0.1482 - val_accuracy: 0.9749\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "# Imports\n",
    "# build_model = __import__('1-input').build_model\n",
    "# optimize_model = __import__('2-optimize').optimize_model\n",
    "# one_hot = __import__('3-one_hot').one_hot\n",
    "# train_model = __import__('5-train').train_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0bb07",
   "metadata": {},
   "source": [
    "# 6. Early Stopping\n",
    "Based on `5-train.py`, update the function `def train_model(network, data, labels, batch_size, epochs, validation_data=None, early_stopping=False, patience=0, verbose=True, shuffle=False):` to also train the model using early stopping:\n",
    "\n",
    "* `early_stopping` is a `boolean` that indicates whether early stopping should be used\n",
    "    * early stopping should only be performed if validation_data exists\n",
    "    * early stopping should be based on validation loss\n",
    "* patience is the patience used for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0e80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that trins a model using mini-batch gradien descent\n",
    "'''\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False, patience=0,\n",
    "                verbose=True, shuffle=False):\n",
    "    '''\n",
    "    Based on 4-train.py, update the function train_model\n",
    "    to also analyze validaiton data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : TYPE model\n",
    "        DESCRIPTION. Model to be train\n",
    "    data : TYPE numpy.ndarray\n",
    "        DESCRIPTION. data is a numpy.ndarray of shape (m, nx) containing\n",
    "        the input data\n",
    "    labels : TYPE numpy.ndarray\n",
    "        DESCRIPTION. (m, classes) containing the labels of data\n",
    "    batch_size : TYPE int\n",
    "        DESCRIPTION. Batch size used for mini-batch gradient descent\n",
    "    epochs : TYPE int\n",
    "        DESCRIPTION. Number of passes through data for mini-batch g.d.\n",
    "    validation_data : TYPE, optional\n",
    "        DESCRIPTION. Dato to validate the model.\n",
    "    early_stopping : TYPE boolean\n",
    "        DESCRIPTION. Indicates if early stopping should be used\n",
    "    patience : TYPE int\n",
    "        DESCRIPTION. is the patience used for early stopping\n",
    "    verbose : TYPE, optional\n",
    "        DESCRIPTION. The default is True.\n",
    "    shuffle : TYPE, optional\n",
    "        DESCRIPTION. The default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    if validation_data:\n",
    "        validation_data = validation_data\n",
    "        early = K.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                          patience=patience)\n",
    "    else:\n",
    "        validation_data = None\n",
    "        early = None\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       validation_data=validation_data,\n",
    "                       shuffle=shuffle,\n",
    "                       callbacks=[early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5f4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.3253 - accuracy: 0.9208 - val_loss: 0.1879 - val_accuracy: 0.9627\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1746 - accuracy: 0.9657 - val_loss: 0.1589 - val_accuracy: 0.9694\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1420 - accuracy: 0.9755 - val_loss: 0.1558 - val_accuracy: 0.9712\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1259 - accuracy: 0.9803 - val_loss: 0.1594 - val_accuracy: 0.9722\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1148 - accuracy: 0.9832 - val_loss: 0.1482 - val_accuracy: 0.9749\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1110 - accuracy: 0.9844 - val_loss: 0.1429 - val_accuracy: 0.9764\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1026 - accuracy: 0.9873 - val_loss: 0.1476 - val_accuracy: 0.9757\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0979 - accuracy: 0.9881 - val_loss: 0.1435 - val_accuracy: 0.9776\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.0998 - accuracy: 0.9876 - val_loss: 0.1428 - val_accuracy: 0.9765\n",
      "Epoch 10/30\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.0951 - accuracy: 0.9885 - val_loss: 0.1378 - val_accuracy: 0.9793\n",
      "Epoch 11/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0938 - accuracy: 0.9892 - val_loss: 0.1457 - val_accuracy: 0.9765\n",
      "Epoch 12/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0915 - accuracy: 0.9900 - val_loss: 0.1371 - val_accuracy: 0.9778\n",
      "Epoch 13/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0910 - accuracy: 0.9904 - val_loss: 0.1439 - val_accuracy: 0.9781\n",
      "Epoch 14/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0873 - accuracy: 0.9905 - val_loss: 0.1381 - val_accuracy: 0.9784\n",
      "Epoch 15/30\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.0905 - accuracy: 0.9895 - val_loss: 0.1296 - val_accuracy: 0.9809\n",
      "Epoch 16/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0845 - accuracy: 0.9911 - val_loss: 0.1345 - val_accuracy: 0.9804\n",
      "Epoch 17/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0845 - accuracy: 0.9912 - val_loss: 0.1312 - val_accuracy: 0.9803\n",
      "Epoch 18/30\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0834 - accuracy: 0.9913 - val_loss: 0.1332 - val_accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "# Imports\n",
    "# build_model = __import__('1-input').build_model\n",
    "# optimize_model = __import__('2-optimize').optimize_model\n",
    "# one_hot = __import__('3-one_hot').one_hot\n",
    "# train_model = __import__('6-train').train_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e03511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

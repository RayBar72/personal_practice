{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47ac6f7",
   "metadata": {},
   "source": [
    "# 0. L2 Regularization Cost\n",
    "Write a function `def l2_reg_cost(cost, lambtha, weights, L, m):` that calculates the cost of a neural network with L2 regularization:\n",
    "\n",
    "- `cost` is the cost of the network without L2 regularization\n",
    "- `lambtha` is the regularization parameter\n",
    "- `weights` is a dictionary of the weights and biases (`numpy.ndarrays`) of the neural network\n",
    "- `L` is the number of layers in the neural network\n",
    "- `m` is the number of data points used\n",
    "- Returns: the cost of the network accounting for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009cc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def l2_reg_cost(cost, lambtha, weights, L, m):\n",
    "    W = [weights['W' + str(i + 1)] for i in range(L)]\n",
    "    W = [np.linalg.norm(w) ** 2 for w in W]\n",
    "    c = cost + lambtha * sum(W) / (2 * m)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868fb2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41842822]\n",
      "[12.11229237]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "\n",
    "    cost = np.abs(np.random.randn(1))\n",
    "\n",
    "    print(cost)\n",
    "    cost = l2_reg_cost(cost, 0.1, weights, 3, 1000)\n",
    "    print(cost)\n",
    "#     W = [weights['W' + str(i + 1)] for i in range(3)]\n",
    "#     print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d34c4",
   "metadata": {},
   "source": [
    "# 1. Gradient Descent with L2 Regularization\n",
    "Write a function `def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):` that updates the weights and biases of a neural network using gradient descent with L2 regularization:\n",
    "\n",
    "* `Y` is a one-hot `numpy.ndarray` of shape (`classes, m`) that contains the correct labels for the data\n",
    "    * `classes` is the number of classes\n",
    "    * `m` is the number of data points\n",
    "* `weights` is a dictionary of the weights and biases of the neural network\n",
    "* `cache` is a dictionary of the outputs of each layer of the neural network\n",
    "* `alpha` is the learning rate\n",
    "* `lambtha` is the L2 regularization parameter\n",
    "* `L` is the number of layers of the network\n",
    "* The neural network uses `tanh` activations on each layer except the last, which uses a `softmax` activation\n",
    "* The weights and biases of the network should be updated in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "107e9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Modulus that updates the weights and biases of a neural\n",
    "network using gradient descent with L2 regularization\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):\n",
    "    '''\n",
    "    Function that updates the weights and biases of a neural\n",
    "    network using gradient descent with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : TYPE numpy.ndarray\n",
    "        DESCRIPTION. Y is a one-hot numpy.ndarray of shape (classes, m)\n",
    "        that contains the correct labels for the data\n",
    "    weights : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the weights and biases of the\n",
    "        neural network\n",
    "    cache : TYPE dictionary\n",
    "        DESCRIPTION. Dictionary of the outputs of each layer of\n",
    "        the neural network\n",
    "    alpha : TYPE float\n",
    "        DESCRIPTION. Learning rate\n",
    "    lambtha : TYPE float\n",
    "        DESCRIPTION. L2 regularization parameter\n",
    "    L : TYPE int\n",
    "        DESCRIPTION. layers of the network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    dz = cache['A' + str(L)] - Y\n",
    "    for i in range(L, 0, -1):\n",
    "        dw = (1 / m) * np.matmul(dz, cache['A' + str(i - 1)].T)\n",
    "        db = (1 / m) * np.sum(dz, axis=1, keepdims=True)\n",
    "        dA = cache['A' + str(i - 1)] * (1 - cache['A' + str(i - 1)])\n",
    "        dz = np.matmul(weights['W' + str(i)].T, dz) * dA\n",
    "        weights['W' + str(i)] = weights[\n",
    "            'W' + str(i)] * (1 - (alpha * lambtha) / m) - (alpha * dw)\n",
    "        weights['b' + str(i)] = weights[\n",
    "            'b' + str(i)] - (alpha * db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4709a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719\n",
      "  -1.34149673]\n",
      " [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133\n",
      "   0.07912172]\n",
      " [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445\n",
      "  -1.07836109]\n",
      " ...\n",
      " [-0.60467085  0.54751161 -1.23317415 ...  0.82895532  1.44161136\n",
      "   0.18972404]\n",
      " [-0.41044606  0.85719512  0.71789835 ... -0.73954771  0.5074628\n",
      "   1.23022874]\n",
      " [ 0.43129249  0.60767018 -0.07749988 ... -0.26611561  2.52287972\n",
      "   0.73131543]]\n",
      "[[ 1.76405199  0.40015713  0.97873779 ...  0.52130364  0.61192707\n",
      "  -1.34149646]\n",
      " [ 0.47689827  0.14844955  0.52904513 ...  0.09600419 -0.04511329\n",
      "   0.07912171]\n",
      " [ 0.85053051 -0.83912402 -1.01177388 ... -0.07223874  0.31112438\n",
      "  -1.07836088]\n",
      " ...\n",
      " [-0.60467073  0.5475115  -1.2331739  ...  0.82895516  1.44161107\n",
      "   0.189724  ]\n",
      " [-0.41044598  0.85719495  0.71789821 ... -0.73954756  0.5074627\n",
      "   1.2302285 ]\n",
      " [ 0.4312924   0.60767006 -0.07749987 ... -0.26611556  2.52287922\n",
      "   0.73131529]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "# l2_reg_gradient_descent = __import__('1-l2_reg_gradient_descent').l2_reg_gradient_descent\n",
    "\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "    cache = {}\n",
    "    cache['A0'] = X_train\n",
    "    cache['A1'] = np.tanh(np.matmul(weights['W1'], cache['A0']) + weights['b1'])\n",
    "    cache['A2'] = np.tanh(np.matmul(weights['W2'], cache['A1']) + weights['b2'])\n",
    "    Z3 = np.matmul(weights['W3'], cache['A2']) + weights['b3']\n",
    "    cache['A3'] = np.exp(Z3) / np.sum(np.exp(Z3), axis=0)\n",
    "    print(weights['W1'])\n",
    "    l2_reg_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.1, 3)\n",
    "    print(weights['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e253a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
